


import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt





df = pd.read_csv("car_prices.csv")
df.head()


df.shape


df.columns


nan_counts = df.isnull().sum()

print("Number of NaNs in each column:")
print(nan_counts)


# Replace NaNs in columns_with_none with 'None'
columns_with_none = ['make', 'model', 'trim', 'body', 'transmission', 'color', 'interior']
df[columns_with_none] = df[columns_with_none].fillna('None')

# Replace NaNs in columns_with_median with the median of each column
columns_with_median = ['condition', 'odometer']
for col in columns_with_median:
    median = df[col].median()
    df[col].fillna(median, inplace=True)


#Convert all columns containing strings to lower case
df = df.map(lambda x: x.lower() if isinstance(x, str) else x)

df = df.map(lambda x: 'coupe' if isinstance(x, str) and 'coupe' in x else x)
df = df.map(lambda x: 'wagon' if isinstance(x, str) and 'wagon' in x else x)
df = df.map(lambda x: 'cab' if isinstance(x, str) and 'cab' in x else x)
df = df.map(lambda x: 'convertible' if isinstance(x, str) and 'convertible' in x else x)
df = df.map(lambda x: 'van' if isinstance(x, str) and 'van' in x else x)
df = df.map(lambda x: 'sedan' if isinstance(x, str) and 'sedan' in x else x)
df = df.map(lambda x: 'coupe' if isinstance(x, str) and 'koup' in x else x)


categorical_columns = df.select_dtypes(include=['object']).columns

# Get unique values for each categorical column
unique_values = {}
for col in categorical_columns:
    unique_values[col] = df[col].unique()

# Print unique values for each categorical column
for col, values in unique_values.items():
    print(f"Unique values in {col}: {values}\n")


brand_counts = df['make'].str.upper().value_counts()

# Plotting histogram horizontally with wider spacing between bars
plt.figure(figsize=(8, 10))  # Adjust figure size as needed
brand_counts.sort_values().plot(kind='barh', color='black', width=0.8)  # Adjust width as needed
plt.title('Histogram of Car Brands')
plt.ylabel('Car Brands')
plt.xlabel('Frequency')
plt.grid(axis='x')  # Adding grid lines along x-axis for better readability
plt.tight_layout()
plt.show()


selling_prices = df['sellingprice']

# Plotting histogram
plt.figure(figsize=(10,6))
plt.hist(selling_prices, bins=30, color='skyblue', edgecolor='black')  # Adjust the number of bins as needed
plt.title('Distribution of Selling Prices')
plt.xlabel('Selling Price')
plt.ylabel('Frequency')
plt.grid(True)  # Adding grid lines for better readability
plt.tight_layout()
plt.show()


mmr = df['mmr']

# Plotting histogram
plt.figure(figsize=(10,6))
plt.hist(mmr, bins=30, color='green', edgecolor='black') 
plt.title('Distribution of Estimated Market Value')
plt.xlabel('MMR')
plt.ylabel('Frequency')
plt.grid(True)
plt.tight_layout()
plt.show()


brand_counts = df['body'].str.upper().value_counts()

# Plotting histogram horizontally with wider spacing between bars
plt.figure(figsize=(10, 5))  # Adjust figure size as needed
brand_counts.sort_values().plot(kind='barh', color='red', width=0.8)  # Adjust width as needed
plt.title('Histogram of Car Body Types')
plt.ylabel('Car Body Types')
plt.xlabel('Frequency')
plt.grid(axis='x')  # Adding grid lines along x-axis for better readability
plt.tight_layout()
plt.show()





# Sample time format
df.iloc[123]['saledate'].split(' ')


month_dict = {'jan': -0.5, 'feb': -0.4, 'mar': -0.3, 'apr': -0.2, 'may': -0.1, 'jun': 0.0,
             'jul': 0.1, 'aug': 0.2, 'sep': 0.3, 'oct': 0.4, 'nov': 0.5, 'dec': 0.6}

def date_to_num(date):
    date = date.split(' ')
    return int(date[3]) + month_dict[date[1]]

df['saledate'] = df['saledate'].apply(date_to_num)
df['years_aged'] = df['saledate'] - df['year']
df.drop('saledate', axis=1, inplace=True)
df.head()


# VIN is not needed
df.drop('vin', axis=1, inplace=True)
df.drop('seller', axis=1, inplace=True)
df.head()


from scipy import stats

# Select only the numerical columns
numerical_columns = df.select_dtypes(include=['int', 'float']).columns

# Calculate Z-scores for numerical columns
z_scores = stats.zscore(df[numerical_columns])

# Plot the distribution of Z-scores for each numerical column
plt.figure(figsize=(10, 6))

for i, col in enumerate(numerical_columns):
    plt.subplot(2, len(numerical_columns)//2, i+1)
    plt.hist(z_scores[z_scores.columns[i]], bins=20, color='skyblue', edgecolor='black')
    plt.title(f'Distribution of Z-scores for {col}')
    plt.xlabel('Z-score')
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()


# Define a threshold for the absolute Z-score value
threshold = 2
 
# Find outliers based on the absolute Z-score
outliers = (abs(z_scores) > threshold).any(axis=1)

# Remove outliers from the DataFrame
df = df[~outliers]
df.shape


X = df.drop('sellingprice', axis=1)
y = df['sellingprice']


from sklearn.preprocessing import OneHotEncoder, MinMaxScaler

categorical_columns = ['make', 'model', 'trim', 'body', 'transmission', 'state', 'color', 'interior']
numerical_columns = ['year', 'condition', 'odometer', 'mmr', 'years_aged']

# One-hot encoding for each categorical column
encoded_dfs = []
for col in categorical_columns:
    encoder = OneHotEncoder(sparse_output=False)
    encoded_col = encoder.fit_transform(X[[col]])
    encoded_df = pd.DataFrame(encoded_col, columns=encoder.get_feature_names_out([col]))
    encoded_dfs.append(encoded_df)

# Standard scaling for each numerical column
scaled_dfs = []
for col in numerical_columns:
    scaler = MinMaxScaler()
    scaled_col = scaler.fit_transform(X[[col]])
    scaled_df = pd.DataFrame(scaled_col, columns=[col])
    scaled_dfs.append(scaled_df)

# Concatenate the encoded and scaled DataFrames
X_xgb = pd.concat(encoded_dfs + scaled_dfs, axis=1)
X_xgb.head()


from sklearn.preprocessing import OrdinalEncoder

categorical_columns = ['make', 'model', 'trim', 'body', 'transmission', 'state', 'color', 'interior']
numerical_columns = ['year', 'condition', 'odometer', 'mmr', 'years_aged']

# Ordinal encoding for each categorical column
encoded_dfs = []
for col in categorical_columns:
    encoder = OrdinalEncoder()
    encoded_col = encoder.fit_transform(X[[col]])
    encoded_df = pd.DataFrame(encoded_col, columns=[col])
    encoded_dfs.append(encoded_df)

# Standard scaling for each numerical column
scaled_dfs = []
for col in numerical_columns:
    scaler = MinMaxScaler()
    scaled_col = scaler.fit_transform(X[[col]])
    scaled_df = pd.DataFrame(scaled_col, columns=[col])
    scaled_dfs.append(scaled_df)

# Concatenate the encoded and scaled DataFrames
X_rf = pd.concat(encoded_dfs + scaled_dfs, axis=1)
X_rf.head()





from sklearn.model_selection import GridSearchCV, train_test_split
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_xgb, y, test_size=0.3, random_state=42)


# Define the parameter grid
param_grid = {
    'learning_rate': [0.15],
    'max_depth': [4],
    'n_estimators': [100],
}
"""
param_grid = {
    'learning_rate': [0.05, 0.1, 0.15],
    'max_depth': [3, 4, 5],
    'n_estimators': [50, 75, 100],
}
"""

# Initialize the XGBoost regressor
xgb = XGBRegressor()

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=3)

# Fit GridSearchCV to the data
grid_search.fit(X_train, y_train)

# Best parameters found
best_params = grid_search.best_params_
print("Best Parameters:", best_params)

# Use the best model for predictions
best_model = grid_search.best_estimator_
pred_xgb = best_model.predict(X_test)

# Calculating RMSE
rmse = mean_squared_error(y_test, pred_xgb, squared=False)
print("Root Mean Squared Error:", rmse)

plt.figure(figsize=(8, 6))
plt.scatter(y_test, pred_xgb, color='blue', alpha=0.5)
plt.title('(XGBoost) Predictions vs. True Values')
plt.xlabel('True Values')
plt.ylabel('Predictions')
plt.grid(True)
plt.show()

rmse = mean_squared_error(y_test, X_test['mmr'], squared=False)
print("Root Mean Squared Error(Directly using MMR): ", rmse)
plt.figure(figsize=(8, 6))
plt.scatter(y_test, X_test['mmr'], color='skyblue', alpha=0.5)
plt.title('(XGBoost) MMR vs. True Values')
plt.xlabel('True Values')
plt.ylabel('MMR\'s')
plt.grid(True)
plt.show()


from sklearn.ensemble import RandomForestRegressor

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_rf, y, test_size=0.3, random_state=42)

rf_param_grid = {
    'n_estimators': [100],
    'max_depth': [30],
    'min_samples_split': [12],
    'min_samples_leaf': [4]
}
"""
rf_param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [30, 40, 50],
    'min_samples_split': [8, 12],
    'min_samples_leaf': [4, 5]
}
"""

rf_model = RandomForestRegressor(random_state=42)
rf_grid_search = GridSearchCV(rf_model, rf_param_grid, cv=3, scoring='neg_mean_squared_error', verbose=3, n_jobs=-1)
rf_grid_search.fit(X_train, y_train)

best_rf_params = rf_grid_search.best_params_
print("Best Parameters:", best_rf_params)

best_rf_model = rf_grid_search.best_estimator_
best_rf_model.fit(X_train, y_train)

pred_rf = best_rf_model.predict(X_test)
rmse_rf = mean_squared_error(y_test, pred_rf, squared=False)
print(f'Random Forest Root Mean Squared Error: {rmse_rf}')

plt.figure(figsize=(8, 6))
plt.scatter(y_test, pred_rf, color='blue', alpha=0.5)
plt.title('(Random Forest) Predictions vs. True Values')
plt.xlabel('True Values')
plt.ylabel('Predictions')
plt.grid(True)
plt.show()



